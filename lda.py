# -*- coding: utf-8 -*-
"""LDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n5JwO6jSs-9JU8hdiZEigLfv9wmfuE4L
"""

# Dependencies.
import numpy as np
from PIL import Image
from google.colab import drive
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import matplotlib.pyplot as plt

"""# **<center>1. Download the dataset and understand the format.</center>**"""

# Mount the Google drive into the runtime.
drive.mount('/content/gdrive')

"""# **<center>2. Generate the data matrix D and the labels vector y.</center>**"""

# Store all images as vectors.
dataset = []

# The ORL dataset has 40 subjects each having 10 images.
for i in range(1, 41):
  for j in range(1, 11):
  
    # Each subject has a folder containing its images.
    folder_path = '/content/gdrive/My Drive/att-database-of-faces/s' + str(i) + '/'
    
    # Convert each image into a 10304-dimensional vector.
    img = Image.open(folder_path + str(j) + '.pgm')
    vec = np.array(img).reshape(10304)
    
    dataset.append(vec)
    
    
# Generate the data matrix D.
D = np.array([example for example in dataset])
D = D.reshape((400, 10304))

# Generate the label vector y.
# The key point here is that we know that the data in the data matrix is 
# ordered, i.e. all the images with label 1 come before those with label 2 
# and so on.
labels = []

for i in range(1, 41):
  labels.append(i * np.ones((10, 1)))
  
y = np.stack(labels, axis=0).reshape(400, 1)

"""# **<center>3. Split the dataset into training and test sets.</center>**"""

# Keep the odd rows for training.
X_train = D[::2]
y_train = y[::2]

# And the even rows for testing.
X_test = D[1::2]
y_test = y[1::2]

"""# **<center>5. Classification using LDA.</center>**"""

# Linear discriminant analysis algorithm.
def LinearDiscriminant(D, y):
  
  # Parition the dataset into class-specific subsets.
  print("[1] Partioning the dataset into class-specific subsets...")
  classes = []
  for label in range(1, 41):
    classes.append([D[i] for i in range(400) if y[i]==label])
    #print("[1] C %s: %s" % (label, len(classes[label-1])))
  classes = np.array(classes)

  # For each class, compute the d-dimensional mean vector.
  print("[2] Computing the mean vector for each class...")
  mean_vecs = np.zeros((10304, 40))
  for i in range(40):
    mean_vecs[:, i] = np.mean(classes[i], axis=0)
    #print("[2] MV %s: %s" % (i+1, mean_vecs[:, i]))

  # Calculate the between-class scatter matrix S_b.
  print("[3] Computing the between-class scatter matrix...")
  mean_overall = np.mean(D, axis=0)
  d = len(D[0])
  S_b = np.zeros((d, d))
  for k in range(classes.shape[0]):
    S_b += len(classes[k]) * (mean_vecs[:, k] - mean_overall).dot(np.transpose(mean_vecs[:, k] - mean_overall))
  print("[3] Between-class scatter matrix: %sx%s" % (S_b.shape[0], S_b.shape[1]))
  
  # Center the class matrices.
  print("[4] Centering the dataset...")
  sc = StandardScaler()
  classes_std = []
  for i in range(40):
    #classes_std.append(classes[i] - mean_vecs[:, i].reshape(1, d))
    classes_std.append(sc.fit_transform(classes[i]))
  
  # Calculate the within-class scatter matrix S_w.
  print("[5] Computing the within-class scatter matrix...")
  S_w = np.zeros((d, d), dtype='float64') # Initialize the total scatter matrix.
  for c in range(40): # For each class,
    class_scatter = np.zeros((d, d)) # initialize the class-specific scatter matrix.
    for i in range(10): # For each example in the class,
      x = classes_std[c][i].reshape(d, 1) # get the example,
      mc = mean_vecs[:,c].reshape(d, 1) # get the class mean,
      class_scatter += (x - mc).dot((x - mc).T) # compute the class-specific scatter matrix.
    S_w += class_scatter
  print("[5] Within-class scatter matrix: (%s x %s) %s" % (S_w.shape[0], S_w.shape[1], S_w.dtype))
  
  # Compute the dominant eigenvectors.
  print("[6] Solving for the eigenvectors...")
  #print("[6] Determinant of S_w: %s" % (np.linalg.det(S_w)))
  eigen_vals, eigen_vecs = np.linalg.eigh(np.linalg.pinv(S_w).dot(S_b))
  
  # Make a list of (eigenvalue, eigenvector) tuples
  eig_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i].real) for i in range(len(eigen_vals))]

  # Sort the (eigenvalue, eigenvector) tuples from high to low
  eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)

  # Visually confirm that the list is correctly sorted by decreasing eigenvalues
  print('[6] Eigenvalues in descending order:')
  for i in eig_pairs:
      print(i[0])
      
  # Use 39 dominant eigenvectors to form the projection matrix.
  U = np.hstack((eig_pairs[0][1][:,np.newaxis],
                 eig_pairs[1][1][:,np.newaxis],
                 eig_pairs[2][1][:,np.newaxis],
                 eig_pairs[3][1][:,np.newaxis],
                 eig_pairs[4][1][:,np.newaxis],
                 eig_pairs[5][1][:,np.newaxis],
                 eig_pairs[6][1][:,np.newaxis],
                 eig_pairs[7][1][:,np.newaxis],
                 eig_pairs[8][1][:,np.newaxis],
                 eig_pairs[9][1][:,np.newaxis],
                 eig_pairs[10][1][:,np.newaxis],
                 eig_pairs[11][1][:,np.newaxis],
                 eig_pairs[12][1][:,np.newaxis],
                 eig_pairs[13][1][:,np.newaxis],
                 eig_pairs[14][1][:,np.newaxis],
                 eig_pairs[15][1][:,np.newaxis],
                 eig_pairs[16][1][:,np.newaxis],
                 eig_pairs[17][1][:,np.newaxis],
                 eig_pairs[18][1][:,np.newaxis],
                 eig_pairs[19][1][:,np.newaxis],
                 eig_pairs[20][1][:,np.newaxis],
                 eig_pairs[21][1][:,np.newaxis],
                 eig_pairs[22][1][:,np.newaxis],
                 eig_pairs[23][1][:,np.newaxis],
                 eig_pairs[24][1][:,np.newaxis],
                 eig_pairs[25][1][:,np.newaxis],
                 eig_pairs[26][1][:,np.newaxis],
                 eig_pairs[27][1][:,np.newaxis],
                 eig_pairs[28][1][:,np.newaxis],
                 eig_pairs[29][1][:,np.newaxis],
                 eig_pairs[30][1][:,np.newaxis],
                 eig_pairs[31][1][:,np.newaxis],
                 eig_pairs[32][1][:,np.newaxis],
                 eig_pairs[33][1][:,np.newaxis],
                 eig_pairs[34][1][:,np.newaxis],
                 eig_pairs[35][1][:,np.newaxis],
                 eig_pairs[36][1][:,np.newaxis],
                 eig_pairs[37][1][:,np.newaxis],
                 eig_pairs[38][1][:,np.newaxis]
                ))
  
  print('[6] Matrix U: \n', U)
  return U, S_w

U, S_w = LinearDiscriminant(D, y)

print(S_w)

"""***Data Projection***"""

# Project the training set.
projection_train = X_train.dot(U)
print(projection_train[0])
# Project the testing set.
projection_test = X_test.dot(U)

"""***kNN Classifier***"""

# Use a first Nearest Neighbor classifier to classify the data.
classifier = KNeighborsClassifier(n_neighbors=1)
classifier.fit(projection_train, y_train) 
y_pred = classifier.predict(projection_test)
print(metrics.accuracy_score(y_test, y_pred))

error = []

for i in range(1, 40):
    classifier = KNeighborsClassifier(n_neighbors=i)
    classifier.fit(projection_train, y_train)
    y_pred = classifier.predict(projection_test)
    error.append(metrics.accuracy_score(y_test, y_pred))

plt.figure(figsize=(12, 6))
plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Accuracy vs. k Hyperparameter')
plt.xlabel('k Value')
plt.ylabel('Accuracy')

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
lda = LinearDiscriminantAnalysis(n_components=39)
X_train_lda = lda.fit_transform(X_train, y_train)

# Use a first Nearest Neighbor classifier to classify the data.
classifier = KNeighborsClassifier(n_neighbors=1)
classifier.fit(X_train_lda, y_train) 
y_pred = classifier.predict(projection_test)
print(metrics.accuracy_score(y_test, y_pred))

error = []

for i in range(1, 40):
    classifier = KNeighborsClassifier(n_neighbors=i)
    classifier.fit(projection_train, y_train)
    y_pred = classifier.predict(projection_test)
    error.append(metrics.accuracy_score(y_test, y_pred))

plt.figure(figsize=(12, 6))
plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',
         markerfacecolor='blue', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')